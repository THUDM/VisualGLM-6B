# VisualGLM-6B

<p align="center">
ğŸ¤— <a href="https://huggingface.co/THUDM/visualglm-6b" target="_blank">HF Repo</a> â€¢ âš’ï¸ <a href="https://github.com/THUDM/SwissArmyTransformer" target="_blank">SwissArmyTransformer (sat)</a> â€¢ ğŸ¦ <a href="https://twitter.com/thukeg" target="_blank">Twitter</a> 
</p>
<p align="center">
â€¢  ğŸ“ƒ <a href="https://arxiv.org/abs/2105.13290" target="_blank">[CogView@NeurIPS 21]</a>  <a href="https://github.com/THUDM/CogView" target="_blank">[GitHub]</a> â€¢ ğŸ“ƒ <a href="https://arxiv.org/abs/2103.10360" target="_blank">[GLM@ACL 22]</a> <a href="https://github.com/THUDM/GLM" target="_blank">[GitHub]</a> <br>
</p>
<p align="center">
    ğŸ‘‹ åŠ å…¥æˆ‘ä»¬çš„ <a href="https://join.slack.com/t/chatglm/shared_invite/zt-1th2q5u69-7tURzFuOPanmuHy9hsZnKA" target="_blank">Slack</a> å’Œ <a href="resources/WECHAT.md" target="_blank">WeChat</a>
</p>
<p align="center">
ğŸ¤–<a href="https://huggingface.co/spaces/THUDM/visualglm-6b" target="_blank">VisualGLM-6Båœ¨çº¿æ¼”ç¤ºç½‘ç«™</a>
</p>

## ä»‹ç»

VisualGLM-6B æ˜¯ä¸€ä¸ªå¼€æºçš„ï¼Œæ”¯æŒ**å›¾åƒã€ä¸­æ–‡å’Œè‹±æ–‡**çš„å¤šæ¨¡æ€å¯¹è¯è¯­è¨€æ¨¡å‹ï¼Œè¯­è¨€æ¨¡å‹åŸºäº [ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B)ï¼Œå…·æœ‰ 62 äº¿å‚æ•°ï¼›å›¾åƒéƒ¨åˆ†é€šè¿‡è®­ç»ƒ [BLIP2-Qformer](https://arxiv.org/abs/2301.12597) æ„å»ºèµ·è§†è§‰æ¨¡å‹ä¸è¯­è¨€æ¨¡å‹çš„æ¡¥æ¢ï¼Œæ•´ä½“æ¨¡å‹å…±78äº¿å‚æ•°ã€‚

VisualGLM-6B ä¾é æ¥è‡ªäº [CogView](https://arxiv.org/abs/2105.13290) æ•°æ®é›†çš„30Mé«˜è´¨é‡ä¸­æ–‡å›¾æ–‡å¯¹ï¼Œä¸300Mç»è¿‡ç­›é€‰çš„è‹±æ–‡å›¾æ–‡å¯¹è¿›è¡Œé¢„è®­ç»ƒï¼Œä¸­è‹±æ–‡æƒé‡ç›¸åŒã€‚è¯¥è®­ç»ƒæ–¹å¼è¾ƒå¥½åœ°å°†è§†è§‰ä¿¡æ¯å¯¹é½åˆ°ChatGLMçš„è¯­ä¹‰ç©ºé—´ï¼›ä¹‹åçš„å¾®è°ƒé˜¶æ®µï¼Œæ¨¡å‹åœ¨é•¿è§†è§‰é—®ç­”æ•°æ®ä¸Šè®­ç»ƒï¼Œä»¥ç”Ÿæˆç¬¦åˆäººç±»åå¥½çš„ç­”æ¡ˆã€‚

VisualGLM-6B ç”± [SwissArmyTransformer](https://github.com/THUDM/SwissArmyTransformer)(ç®€ç§°`sat`) åº“è®­ç»ƒï¼Œè¿™æ˜¯ä¸€ä¸ªæ”¯æŒTransformerçµæ´»ä¿®æ”¹ã€è®­ç»ƒçš„å·¥å…·åº“ï¼Œæ”¯æŒLoraã€P-tuningç­‰å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ã€‚æœ¬é¡¹ç›®æä¾›äº†ç¬¦åˆç”¨æˆ·ä¹ æƒ¯çš„huggingfaceæ¥å£ï¼Œä¹Ÿæä¾›äº†åŸºäºsatçš„æ¥å£ã€‚

ä¸è¿‡ï¼Œç”±äº VisualGLM-6B ä»å¤„äºv1ç‰ˆæœ¬ï¼Œç›®å‰å·²çŸ¥å…¶å…·æœ‰ç›¸å½“å¤šçš„[**å±€é™æ€§**](#å±€é™æ€§)ï¼Œå¦‚å›¾åƒæè¿°äº‹å®æ€§/æ¨¡å‹å¹»è§‰é—®é¢˜ï¼Œå›¾åƒç»†èŠ‚ä¿¡æ¯æ•æ‰ä¸è¶³ï¼Œä»¥åŠä¸€äº›æ¥è‡ªè¯­è¨€æ¨¡å‹çš„å±€é™æ€§ã€‚è¯·å¤§å®¶åœ¨ä½¿ç”¨å‰äº†è§£è¿™äº›é—®é¢˜ï¼Œè¯„ä¼°å¯èƒ½å­˜åœ¨çš„é£é™©ã€‚åœ¨VisualGLMä¹‹åçš„ç‰ˆæœ¬ä¸­ï¼Œå°†ä¼šç€åŠ›å¯¹æ­¤ç±»é—®é¢˜è¿›è¡Œä¼˜åŒ–ã€‚

ç»“åˆæ¨¡å‹é‡åŒ–æŠ€æœ¯ï¼Œç”¨æˆ·å¯ä»¥åœ¨æ¶ˆè´¹çº§çš„æ˜¾å¡ä¸Šè¿›è¡Œæœ¬åœ°éƒ¨ç½²ï¼ˆINT4é‡åŒ–çº§åˆ«ä¸‹æœ€ä½åªéœ€8.7Gæ˜¾å­˜ï¼‰ã€‚

<details>
<summary> VisualGLM-6B is an open-source, multimodal conversational language model that supports <b>images, Chinese, and English</b>.
    Click to expand the English verison. </summary>
<br>
 
VisualGLM-6B is an open-source, multimodal conversational language model that supports **images, Chinese, and English**. The language model is based on [ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B), with 6.2 billion parameters; the image part is bridged to the language model by training [BLIP2-Qformer](https://arxiv.org/abs/2301.12597), making the total model parameters amount to 7.8 billion.

VisualGLM-6B relies on 30 million high-quality Chinese image-text pairs from the [CogView](https://arxiv.org/abs/2105.13290) dataset, and 300 million selected English image-text pairs for pre-training, with equal weights for Chinese and English. This training method aligns visual information well with the semantic space of ChatGLM. In the fine-tuning stage, the model is trained on a long visual question-answering dataset to generate answers that are in line with human preferences.

VisualGLM-6B is trained using the [SwissArmyTransformer](https://github.com/THUDM/SwissArmyTransformer) (short for `sat`) library, a toolkit supporting flexible modification and training of Transformers, as well as efficient parameter fine-tuning methods such as Lora and P-tuning. This project provides an interface that aligns with user habits in huggingface, as well as an interface based on sat.

However, as VisualGLM-6B is still in its v1 version, it is known to have quite a few [**limitations**](#limitations), such as factual/illusion issues in image descriptions, inadequate capture of image detail information, and some limitations from the language model. Please understand these issues before use and evaluate potential risks. These issues will be the focus of optimization in future versions of VisualGLM.

By integrating model quantization technology, users can deploy locally on consumer-grade graphics cards (requiring only 8.7G of video memory at the INT4 quantization level).
</details>

<!-- *Read this in [English](README_en.md). TODO* -->

## æ ·ä¾‹
VisualGLM-6B å¯ä»¥è¿›è¡Œå›¾åƒçš„æè¿°çš„ç›¸å…³çŸ¥è¯†çš„é—®ç­”ã€‚
![æ³°å¦å°¼å…‹å·æ ·ä¾‹](examples/chat_example1.png)

<details>
<summary>ä¹Ÿèƒ½ç»“åˆå¸¸è¯†æˆ–æå‡ºæœ‰è¶£çš„è§‚ç‚¹ï¼Œç‚¹å‡»å±•å¼€/æŠ˜å æ›´å¤šæ ·ä¾‹</summary>

![å‡ºç§Ÿè½¦ç†¨è¡£æœæ ·ä¾‹](examples/chat_example2.png)
![è’™å¨œä¸½èç‹—æ ·ä¾‹](examples/chat_example3.png)

</details>


## ä½¿ç”¨

### æ¨¡å‹æ¨ç†

ä½¿ç”¨pipå®‰è£…ä¾èµ–
```
pip install -r requirements.txt
```
å°½é‡ä½¿ç”¨æ ‡å‡†PyPIæºä»¥ä¸‹è½½è¾ƒæ–°çš„satåŒ…ï¼ŒTUNAæºç­‰å¯èƒ½åŒæ­¥è¾ƒæ…¢ã€‚`pip install -i https://pypi.org/simple -r requirements.txt`ã€‚
æ­¤æ—¶é»˜è®¤ä¼šå®‰è£…`deepspeed`åº“ï¼ˆæ”¯æŒ`sat`åº“è®­ç»ƒï¼‰ï¼Œæ­¤åº“å¯¹äºæ¨¡å‹æ¨ç†å¹¶éå¿…è¦ï¼ŒåŒæ—¶éƒ¨åˆ†Windowsç¯å¢ƒå®‰è£…æ­¤åº“æ—¶ä¼šé‡åˆ°é—®é¢˜ã€‚å¦‚æœæƒ³ç»•è¿‡`deepspeed`å®‰è£…ï¼Œæˆ‘ä»¬å¯ä»¥å°†å‘½ä»¤æ”¹ä¸º
```
pip install -r requirements_wo_ds.txt
pip install --no-deps 'SwissArmyTransformer>=0.3.6'
```

å¦‚æœä½¿ç”¨Huggingface transformersåº“è°ƒç”¨æ¨¡å‹ï¼Œå¯ä»¥é€šè¿‡å¦‚ä¸‹ä»£ç ï¼ˆå…¶ä¸­å›¾åƒè·¯å¾„ä¸ºæœ¬åœ°è·¯å¾„ï¼‰ï¼š
```python
from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained("THUDM/visualglm-6b", trust_remote_code=True)
model = AutoModel.from_pretrained("THUDM/visualglm-6b", trust_remote_code=True).half().cuda()
image_path = "your image path"
response, history = model.chat(tokenizer, image_path, "æè¿°è¿™å¼ å›¾ç‰‡ã€‚", history=[])
print(response)
response, history = model.chat(tokenizer, "è¿™å¼ å›¾ç‰‡å¯èƒ½æ˜¯åœ¨ä»€ä¹ˆåœºæ‰€æ‹æ‘„çš„ï¼Ÿ", history=history)
print(response)
```

å¦‚æœä½¿ç”¨SwissArmyTransformeråº“è°ƒç”¨æ¨¡å‹ï¼Œæ–¹æ³•ç±»ä¼¼ï¼Œå¯ä»¥ä½¿ç”¨ç¯å¢ƒå˜é‡`SAT_HOME`å†³å®šæ¨¡å‹ä¸‹è½½ä½ç½®ã€‚åœ¨æœ¬ä»“åº“ç›®å½•ä¸‹ï¼š
```python
>>> import argparse
>>> from transformers import AutoTokenizer
>>> tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True)
>>> from model import chat, VisualGLMModel
>>> model, model_args = VisualGLMModel.from_pretrained('visualglm-6b', args=argparse.Namespace(fp16=True, skip_init=True))
>>> from sat.model.mixins import CachedAutoregressiveMixin
>>> model.add_mixin('auto-regressive', CachedAutoregressiveMixin())
>>> image_path = "your image path or URL"
>>> response, history, cache_image = chat(image_path, model, tokenizer, "æè¿°è¿™å¼ å›¾ç‰‡ã€‚", history=[])
>>> print(response)
>>> response, history, cache_image = chat(None, model, tokenizer, "è¿™å¼ å›¾ç‰‡å¯èƒ½æ˜¯åœ¨ä»€ä¹ˆåœºæ‰€æ‹æ‘„çš„ï¼Ÿ", history=history, image=cache_image)
>>> print(response)
```
ä½¿ç”¨`sat`åº“ä¹Ÿå¯ä»¥è½»æ¾è¿›è¡Œè¿›è¡Œå‚æ•°é«˜æ•ˆå¾®è°ƒã€‚<!-- TODO å…·ä½“ä»£ç  -->

è¯·æ³¨æ„ï¼Œ`Huggingface`æ¨¡å‹çš„å®ç°ä½äº[Huggingfaceçš„ä»“åº“](https://huggingface.co/THUDM/visualglm-6b)ä¸­ï¼Œ`sat`æ¨¡å‹çš„å®ç°åŒ…å«äºæœ¬ä»“åº“ä¸­ã€‚

## éƒ¨ç½²å·¥å…·

### å‘½ä»¤è¡Œ Demo

```shell
python cli_demo.py 
```
ç¨‹åºä¼šè‡ªåŠ¨ä¸‹è½½satæ¨¡å‹ï¼Œå¹¶åœ¨å‘½ä»¤è¡Œä¸­è¿›è¡Œäº¤äº’å¼çš„å¯¹è¯ï¼Œè¾“å…¥æŒ‡ç¤ºå¹¶å›è½¦å³å¯ç”Ÿæˆå›å¤ï¼Œè¾“å…¥ clear å¯ä»¥æ¸…ç©ºå¯¹è¯å†å²ï¼Œè¾“å…¥ stop ç»ˆæ­¢ç¨‹åºã€‚

![cli_demo](examples/thu.png)
ç¨‹åºæä¾›å¦‚ä¸‹è¶…å‚æ•°æ§åˆ¶ç”Ÿæˆè¿‡ç¨‹ä¸é‡åŒ–ç²¾åº¦ï¼š
```
usage: cli_demo.py [-h] [--max_length MAX_LENGTH] [--top_p TOP_P] [--top_k TOP_K] [--temperature TEMPERATURE] [--english] [--quant {8,4}]

optional arguments:
  -h, --help            show this help message and exit
  --max_length MAX_LENGTH
                        max length of the total sequence
  --top_p TOP_P         top p for nucleus sampling
  --top_k TOP_K         top k for top k sampling
  --temperature TEMPERATURE
                        temperature for sampling
  --english             only output English
  --quant {8,4}         quantization bits
```
éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œåœ¨è®­ç»ƒæ—¶è‹±æ–‡é—®ç­”å¯¹çš„æç¤ºè¯ä¸º`Q: A:`ï¼Œè€Œä¸­æ–‡ä¸º`é—®ï¼šç­”ï¼š`ï¼Œåœ¨ç½‘é¡µdemoä¸­é‡‡å–äº†ä¸­æ–‡çš„æç¤ºï¼Œå› æ­¤è‹±æ–‡å›å¤ä¼šå·®ä¸€äº›ä¸”å¤¹æ‚ä¸­æ–‡ï¼›å¦‚æœéœ€è¦è‹±æ–‡å›å¤ï¼Œè¯·ä½¿ç”¨`cli_demo.py`ä¸­çš„`--english`é€‰é¡¹ã€‚

æˆ‘ä»¬ä¹Ÿæä¾›äº†ç»§æ‰¿è‡ª`ChatGLM-6B`çš„æ‰“å­—æœºæ•ˆæœå‘½ä»¤è¡Œå·¥å…·ï¼Œæ­¤å·¥å…·ä½¿ç”¨Huggingfaceæ¨¡å‹ï¼š
```shell
python cli_demo_hf.py
```

### ç½‘é¡µç‰ˆ Demo
![web_demo](examples/web_demo.png)

æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªåŸºäº [Gradio](https://gradio.app) çš„ç½‘é¡µç‰ˆ Demoï¼Œé¦–å…ˆå®‰è£… Gradioï¼š`pip install gradio`ã€‚
ç„¶åä¸‹è½½å¹¶è¿›å…¥æœ¬ä»“åº“è¿è¡Œ`web_demo.py`ï¼š

```
git clone https://github.com/THUDM/VisualGLM-6B
cd VisualGLM-6B
python web_demo.py
```
ç¨‹åºä¼šè‡ªåŠ¨ä¸‹è½½satæ¨¡å‹ï¼Œå¹¶è¿è¡Œä¸€ä¸ª Web Serverï¼Œå¹¶è¾“å‡ºåœ°å€ã€‚åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€è¾“å‡ºçš„åœ°å€å³å¯ä½¿ç”¨ã€‚`--quant 4`ä½¿ç”¨4æ¯”ç‰¹é‡åŒ–å‡å°‘æ˜¾å­˜å ç”¨ã€‚

æˆ‘ä»¬ä¹Ÿæä¾›äº†ç»§æ‰¿è‡ª`ChatGLM-6B`çš„æ‰“å­—æœºæ•ˆæœç½‘é¡µç‰ˆå·¥å…·ï¼Œæ­¤å·¥å…·ä½¿ç”¨Huggingfaceæ¨¡å‹ï¼š
```shell
python web_demo_hf.py
```

### APIéƒ¨ç½²
é¦–å…ˆéœ€è¦å®‰è£…é¢å¤–çš„ä¾èµ– `pip install fastapi uvicorn`ï¼Œç„¶åè¿è¡Œä»“åº“ä¸­çš„ [api.py](api.py)ï¼š
```shell
python api.py
```
ç¨‹åºä¼šè‡ªåŠ¨ä¸‹è½½satæ¨¡å‹ï¼Œé»˜è®¤éƒ¨ç½²åœ¨æœ¬åœ°çš„ 8080 ç«¯å£ï¼Œé€šè¿‡ POST æ–¹æ³•è¿›è¡Œè°ƒç”¨ã€‚ä¸‹é¢æ˜¯ç”¨`curl`è¯·æ±‚çš„ä¾‹å­ï¼Œä¸€èˆ¬è€Œè¨€å¯ä»¥ä¹Ÿå¯ä»¥ä½¿ç”¨ä»£ç æ–¹æ³•è¿›è¡ŒPOSTã€‚
```shell
echo "{\"image\":\"$(base64 path/to/example.jpg)\",\"text\":\"æè¿°è¿™å¼ å›¾ç‰‡\",\"history\":[]}" > temp.json
curl -X POST -H "Content-Type: application/json" -d @temp.json http://127.0.0.1:8080
```
å¾—åˆ°çš„è¿”å›å€¼ä¸º
```
  {
    "response":"è¿™å¼ å›¾ç‰‡å±•ç°äº†ä¸€åªå¯çˆ±çš„å¡é€šç¾Šé©¼ï¼Œå®ƒç«™åœ¨ä¸€ä¸ªé€æ˜çš„èƒŒæ™¯ä¸Šã€‚è¿™åªç¾Šé©¼é•¿ç€ä¸€å¼ æ¯›èŒ¸èŒ¸çš„è€³æœµå’Œä¸€åŒå¤§å¤§çš„çœ¼ç›ï¼Œå®ƒçš„èº«ä½“æ˜¯ç™½è‰²çš„ï¼Œå¸¦æœ‰æ£•è‰²æ–‘ç‚¹ã€‚",
    "history":[('æè¿°è¿™å¼ å›¾ç‰‡', 'è¿™å¼ å›¾ç‰‡å±•ç°äº†ä¸€åªå¯çˆ±çš„å¡é€šç¾Šé©¼ï¼Œå®ƒç«™åœ¨ä¸€ä¸ªé€æ˜çš„èƒŒæ™¯ä¸Šã€‚è¿™åªç¾Šé©¼é•¿ç€ä¸€å¼ æ¯›èŒ¸èŒ¸çš„è€³æœµå’Œä¸€åŒå¤§å¤§çš„çœ¼ç›ï¼Œå®ƒçš„èº«ä½“æ˜¯ç™½è‰²çš„ï¼Œå¸¦æœ‰æ£•è‰²æ–‘ç‚¹ã€‚')],
    "status":200,
    "time":"2023-05-16 20:20:10"
  }
```

## æ¨¡å‹é‡åŒ–
åœ¨Huggingfaceå®ç°ä¸­ï¼Œæ¨¡å‹é»˜è®¤ä»¥ FP16 ç²¾åº¦åŠ è½½ï¼Œè¿è¡Œä¸Šè¿°ä»£ç éœ€è¦å¤§æ¦‚ 15GB æ˜¾å­˜ã€‚å¦‚æœä½ çš„ GPU æ˜¾å­˜æœ‰é™ï¼Œå¯ä»¥å°è¯•ä»¥é‡åŒ–æ–¹å¼åŠ è½½æ¨¡å‹ã€‚
ä½¿ç”¨æ–¹æ³•å¦‚ä¸‹ï¼š
```python
# æŒ‰éœ€ä¿®æ”¹ï¼Œç›®å‰åªæ”¯æŒ 4/8 bit é‡åŒ–ã€‚ä¸‹é¢å°†åªé‡åŒ–ChatGLMï¼ŒViT é‡åŒ–æ—¶è¯¯å·®è¾ƒå¤§
model = AutoModel.from_pretrained("THUDM/visualglm-6b", trust_remote_code=True).quantize(8).half().cuda()
```

åœ¨satå®ç°ä¸­ï¼Œéœ€å…ˆä¼ å‚å°†åŠ è½½ä½ç½®æ”¹ä¸º`cpu`ï¼Œå†è¿›è¡Œé‡åŒ–ã€‚æ–¹æ³•å¦‚ä¸‹ï¼Œè¯¦è§`cli_demo.py`ï¼š
```python
from sat.quantization.kernels import quantize
model = quantize(model.transformer, args.quant).cuda()
# æŒ‡å®š model.transformer åªé‡åŒ– ChatGLMï¼ŒViT é‡åŒ–æ—¶è¯¯å·®è¾ƒå¤§
```

## å±€é™æ€§
æœ¬é¡¹ç›®æ­£å¤„äºV1ç‰ˆæœ¬è§†è§‰å’Œè¯­è¨€æ¨¡å‹çš„å‚æ•°ã€è®¡ç®—é‡éƒ½è¾ƒå°ï¼Œæˆ‘ä»¬æ€»ç»“äº†å¦‚ä¸‹ä¸»è¦å­˜åœ¨çš„æ”¹è¿›æ–¹å‘ï¼š
- å›¾åƒæè¿°äº‹å®æ€§/æ¨¡å‹å¹»è§‰é—®é¢˜ã€‚åœ¨ç”Ÿæˆå›¾åƒé•¿æè¿°çš„æ—¶å€™ï¼Œè·ç¦»å›¾åƒè¾ƒè¿œæ—¶ï¼Œè¯­è¨€æ¨¡å‹çš„å°†å ä¸»å¯¼ï¼Œæœ‰ä¸€å®šå¯èƒ½æ ¹æ®ä¸Šä¸‹æ–‡ç”Ÿæˆå¹¶ä¸å­˜åœ¨äºå›¾åƒçš„å†…å®¹ã€‚
- å±æ€§é”™é…é—®é¢˜ã€‚åœ¨å¤šç‰©ä½“çš„åœºæ™¯ä¸­ï¼Œéƒ¨åˆ†ç‰©ä½“çš„æŸäº›å±æ€§ï¼Œç»å¸¸è¢«é”™è¯¯å®‰æ’åˆ°å…¶ä»–ç‰©ä½“ä¸Šã€‚
- åˆ†è¾¨ç‡é—®é¢˜ã€‚æœ¬é¡¹ç›®ä½¿ç”¨äº†224*224çš„åˆ†è¾¨ç‡ï¼Œä¹Ÿæ˜¯è§†è§‰æ¨¡å‹ä¸­æœ€ä¸ºå¸¸ç”¨çš„å°ºå¯¸ï¼›ç„¶è€Œä¸ºäº†è¿›è¡Œæ›´ç»†ç²’åº¦çš„ç†è§£ï¼Œæ›´å¤§çš„åˆ†è¾¨ç‡å’Œè®¡ç®—é‡æ˜¯å¿…è¦çš„ã€‚
- ç”±äºæ•°æ®ç­‰æ–¹é¢åŸå› ï¼Œæ¨¡å‹æš‚æ—¶ä¸å…·æœ‰ä¸­æ–‡ocrçš„èƒ½åŠ›ï¼ˆè‹±æ–‡ocrèƒ½åŠ›æœ‰ä¸€äº›ï¼‰ï¼Œæˆ‘ä»¬ä¼šåœ¨åç»­ç‰ˆæœ¬ä¸­å¢åŠ è¿™ä¸ªèƒ½åŠ›ã€‚
## åè®®

æœ¬ä»“åº“çš„ä»£ç ä¾ç…§ [Apache-2.0](LICENSE) åè®®å¼€æºï¼ŒVisualGLM-6B æ¨¡å‹çš„æƒé‡çš„ä½¿ç”¨åˆ™éœ€è¦éµå¾ª [Model License](MODEL_LICENSE)ã€‚

## å¼•ç”¨ä¸è‡´è°¢
å¦‚æœä½ è§‰å¾—æˆ‘ä»¬çš„å·¥ä½œæœ‰å¸®åŠ©çš„è¯ï¼Œè¯·è€ƒè™‘å¼•ç”¨ä¸‹åˆ—è®ºæ–‡
```
@inproceedings{du2022glm,
  title={GLM: General Language Model Pretraining with Autoregressive Blank Infilling},
  author={Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang, Jie},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={320--335},
  year={2022}
}
@article{ding2021cogview,
  title={Cogview: Mastering text-to-image generation via transformers},
  author={Ding, Ming and Yang, Zhuoyi and Hong, Wenyi and Zheng, Wendi and Zhou, Chang and Yin, Da and Lin, Junyang and Zou, Xu and Shao, Zhou and Yang, Hongxia and others},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={19822--19835},
  year={2021}
}
```
åœ¨VisualGLM-6Bçš„æŒ‡ä»¤å¾®è°ƒé˜¶æ®µçš„æ•°æ®é›†ä¸­ï¼ŒåŒ…å«äº†æ¥è‡ª[MiniGPT-4](https://github.com/Vision-CAIR/MiniGPT-4)å’Œ[LLAVA](https://github.com/haotian-liu/LLaVA)é¡¹ç›®çš„ä¸€éƒ¨åˆ†è‹±æ–‡å›¾æ–‡æ•°æ®ï¼Œä»¥åŠè®¸å¤šç»å…¸çš„è·¨æ¨¡æ€å·¥ä½œæ•°æ®é›†ï¼Œè¡·å¿ƒæ„Ÿè°¢ä»–ä»¬çš„è´¡çŒ®ã€‚
